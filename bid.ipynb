{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fichier prétraité enregistré sous : vehicles_processed.csv\n",
      "X dtype avant conversion: float64\n",
      "y dtype avant conversion: object\n",
      "NaN values in X: 0\n",
      "NaN values in y: 0\n",
      "Sample of X_scaled data after scaling: [[[0.8065262  0.83099365]\n",
      "  [0.80677795 0.8298645 ]\n",
      "  [0.80677795 0.8298645 ]\n",
      "  [0.80677795 0.8298645 ]\n",
      "  [0.80677795 0.8298645 ]\n",
      "  [0.80677795 0.8298645 ]\n",
      "  [0.80677795 0.8298645 ]\n",
      "  [0.80677795 0.8298645 ]\n",
      "  [0.80677795 0.8298645 ]\n",
      "  [0.80677795 0.8298645 ]]\n",
      "\n",
      " [[0.80677795 0.8298645 ]\n",
      "  [0.80677795 0.8298645 ]\n",
      "  [0.80677795 0.8298645 ]\n",
      "  [0.80677795 0.8298645 ]\n",
      "  [0.80677795 0.8298645 ]\n",
      "  [0.80677795 0.8298645 ]\n",
      "  [0.80677795 0.8298645 ]\n",
      "  [0.80677795 0.8298645 ]\n",
      "  [0.80677795 0.8298645 ]\n",
      "  [0.80493164 0.8371887 ]]\n",
      "\n",
      " [[0.80677795 0.8298645 ]\n",
      "  [0.80677795 0.8298645 ]\n",
      "  [0.80677795 0.8298645 ]\n",
      "  [0.80677795 0.8298645 ]\n",
      "  [0.80677795 0.8298645 ]\n",
      "  [0.80677795 0.8298645 ]\n",
      "  [0.80677795 0.8298645 ]\n",
      "  [0.80677795 0.8298645 ]\n",
      "  [0.80493164 0.8371887 ]\n",
      "  [0.81614685 0.8417053 ]]\n",
      "\n",
      " [[0.80677795 0.8298645 ]\n",
      "  [0.80677795 0.8298645 ]\n",
      "  [0.80677795 0.8298645 ]\n",
      "  [0.80677795 0.8298645 ]\n",
      "  [0.80677795 0.8298645 ]\n",
      "  [0.80677795 0.8298645 ]\n",
      "  [0.80677795 0.8298645 ]\n",
      "  [0.80493164 0.8371887 ]\n",
      "  [0.81614685 0.8417053 ]\n",
      "  [0.82434845 0.8215332 ]]\n",
      "\n",
      " [[0.80677795 0.8298645 ]\n",
      "  [0.80677795 0.8298645 ]\n",
      "  [0.80677795 0.8298645 ]\n",
      "  [0.80677795 0.8298645 ]\n",
      "  [0.80677795 0.8298645 ]\n",
      "  [0.80677795 0.8298645 ]\n",
      "  [0.80493164 0.8371887 ]\n",
      "  [0.81614685 0.8417053 ]\n",
      "  [0.82434845 0.8215332 ]\n",
      "  [0.83045197 0.7970886 ]]]\n",
      "X_train dtype après normalisation: float32\n",
      "y_train dtype après normalisation: float32\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yidir\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\layers\\rnn\\bidirectional.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3052/3052\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 6ms/step - loss: 1075.8793 - val_loss: 0.0028\n",
      "Epoch 2/5\n",
      "\u001b[1m3052/3052\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 6ms/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 3/5\n",
      "\u001b[1m3052/3052\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 6ms/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 4/5\n",
      "\u001b[1m3052/3052\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 6ms/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 5/5\n",
      "\u001b[1m3052/3052\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 6ms/step - loss: 0.0031 - val_loss: 0.0030\n",
      "\u001b[1m382/382\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (12205,2) (10,) (12205,2) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 116\u001b[0m\n\u001b[0;32m    113\u001b[0m y_pred_reshaped \u001b[38;5;241m=\u001b[39m y_pred\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)  \u001b[38;5;66;03m# Applatir y_pred\u001b[39;00m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;66;03m# Inverser la normalisation\u001b[39;00m\n\u001b[1;32m--> 116\u001b[0m y_test_actual \u001b[38;5;241m=\u001b[39m \u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minverse_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_test_reshaped\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    117\u001b[0m y_pred_actual \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39minverse_transform(y_pred_reshaped)\n\u001b[0;32m    119\u001b[0m \u001b[38;5;66;03m# Affichage des résultats (latitude vs longitude)\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\preprocessing\\_data.py:572\u001b[0m, in \u001b[0;36mMinMaxScaler.inverse_transform\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    563\u001b[0m xp, _ \u001b[38;5;241m=\u001b[39m get_namespace(X)\n\u001b[0;32m    565\u001b[0m X \u001b[38;5;241m=\u001b[39m check_array(\n\u001b[0;32m    566\u001b[0m     X,\n\u001b[0;32m    567\u001b[0m     copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy,\n\u001b[0;32m    568\u001b[0m     dtype\u001b[38;5;241m=\u001b[39m_array_api\u001b[38;5;241m.\u001b[39msupported_float_dtypes(xp),\n\u001b[0;32m    569\u001b[0m     force_all_finite\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow-nan\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    570\u001b[0m )\n\u001b[1;32m--> 572\u001b[0m \u001b[43mX\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmin_\u001b[49m\n\u001b[0;32m    573\u001b[0m X \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale_\n\u001b[0;32m    574\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m X\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (12205,2) (10,) (12205,2) "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Bidirectional\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Charger le fichier CSV\n",
    "file_path = 'vehicles1.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Filtrage des colonnes nécessaires\n",
    "filtered_data = data[['vehicle_id', 'timestamp', 'latitude', 'longitude']]\n",
    "\n",
    "# Suppression des doublons\n",
    "filtered_data = filtered_data.drop_duplicates(subset=['vehicle_id', 'latitude', 'longitude', 'timestamp'])\n",
    "\n",
    "# Gestion des valeurs manquantes\n",
    "filtered_data = filtered_data.dropna()  # Supprimer les lignes avec des valeurs manquantes\n",
    "\n",
    "# Conversion des timestamps en format datetime\n",
    "filtered_data['timestamp'] = pd.to_datetime(filtered_data['timestamp'], errors='coerce')\n",
    "\n",
    "# Suppression des lignes où la conversion du timestamp a échoué\n",
    "filtered_data = filtered_data.dropna(subset=['timestamp'])\n",
    "\n",
    "# Tri des données par vehicle_id et timestamp\n",
    "filtered_data = filtered_data.sort_values(by=['vehicle_id', 'timestamp'])\n",
    "\n",
    "# Sauvegarde des données prétraitées dans un nouveau fichier CSV\n",
    "output_file_path = 'vehicles_processed.csv'\n",
    "filtered_data.to_csv(output_file_path, index=False)\n",
    "\n",
    "print(f\"Fichier prétraité enregistré sous : {output_file_path}\")\n",
    "\n",
    "# Fonction pour créer des séquences\n",
    "def create_sequences(df, sequence_length=10):\n",
    "    sequences = []\n",
    "    labels = []\n",
    "\n",
    "    # Regrouper les données par véhicule\n",
    "    for vehicle_id in df['vehicle_id'].unique():\n",
    "        vehicle_data = df[df['vehicle_id'] == vehicle_id].sort_values('timestamp')\n",
    "        \n",
    "        # Créer des séquences de longueur 'sequence_length'\n",
    "        for i in range(len(vehicle_data) - sequence_length):\n",
    "            sequence = vehicle_data.iloc[i:i+sequence_length][['latitude', 'longitude']].values\n",
    "            label = vehicle_data.iloc[i + sequence_length][['latitude', 'longitude']].values\n",
    "            sequences.append(sequence)\n",
    "            labels.append(label)\n",
    "    \n",
    "    return np.array(sequences), np.array(labels)\n",
    "\n",
    "# Créer les séquences avec les données filtrées\n",
    "X, y = create_sequences(filtered_data)\n",
    "\n",
    "# Vérification des types de données dans X et y\n",
    "print(f\"X dtype avant conversion: {X.dtype}\")\n",
    "print(f\"y dtype avant conversion: {y.dtype}\")\n",
    "\n",
    "# Conversion en float32 si nécessaire\n",
    "X = X.astype('float32')\n",
    "y = y.astype('float32')\n",
    "\n",
    "# Vérification des valeurs NaN ou infinies\n",
    "print(f\"NaN values in X: {np.isnan(X).sum()}\")\n",
    "print(f\"NaN values in y: {np.isnan(y).sum()}\")\n",
    "\n",
    "# Remplacer les NaN par des zéros (si nécessaire)\n",
    "X = np.nan_to_num(X)\n",
    "y = np.nan_to_num(y)\n",
    "\n",
    "# Normalisation des données (latitude et longitude)\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "X_scaled = np.zeros_like(X)\n",
    "\n",
    "# Appliquer la normalisation sur les 2 dimensions (latitude et longitude) pour toutes les séquences\n",
    "for i in range(X.shape[2]):  # Latitude et Longitude (les 2 dimensions)\n",
    "    X_scaled[:, :, i] = scaler.fit_transform(X[:, :, i])\n",
    "\n",
    "# Vérification des données normalisées\n",
    "print(f\"Sample of X_scaled data after scaling: {X_scaled[:5]}\")\n",
    "\n",
    "# Diviser les données en ensembles d'entraînement et de test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Vérification des types de données dans X_train et y_train après la normalisation\n",
    "print(f\"X_train dtype après normalisation: {X_train.dtype}\")\n",
    "print(f\"y_train dtype après normalisation: {y_train.dtype}\")\n",
    "\n",
    "# Construction du modèle LSTM bidirectionnel\n",
    "model = Sequential()\n",
    "\n",
    "# LSTM bidirectionnel\n",
    "model.add(Bidirectional(LSTM(64, return_sequences=False), input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "\n",
    "# Couche de sortie pour prédire latitude et longitude (2 valeurs)\n",
    "model.add(Dense(2))\n",
    "\n",
    "# Compilation du modèle\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Entraînement du modèle\n",
    "history = model.fit(X_train, y_train, epochs=5, batch_size=16, validation_data=(X_test, y_test))\n",
    "\n",
    "# Prédiction sur les données de test\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Inverse de la normalisation pour revenir aux valeurs réelles\n",
    "# Reshape y_test et y_pred en 2D pour l'inverse de la normalisation\n",
    "y_test_reshaped = y_test.reshape(-1, 2)  # Applatir y_test\n",
    "y_pred_reshaped = y_pred.reshape(-1, 2)  # Applatir y_pred\n",
    "\n",
    "# Inverser la normalisation\n",
    "y_test_actual = scaler.inverse_transform(y_test_reshaped)\n",
    "y_pred_actual = scaler.inverse_transform(y_pred_reshaped)\n",
    "\n",
    "# Affichage des résultats (latitude vs longitude)\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Latitude\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(y_test_actual[:, 0], label='True Latitude')\n",
    "plt.plot(y_pred_actual[:, 0], label='Predicted Latitude', linestyle='--')\n",
    "plt.title('Latitude Prediction')\n",
    "plt.xlabel('Sample Index')\n",
    "plt.ylabel('Latitude')\n",
    "plt.legend()\n",
    "\n",
    "# Longitude\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(y_test_actual[:, 1], label='True Longitude')\n",
    "plt.plot(y_pred_actual[:, 1], label='Predicted Longitude', linestyle='--')\n",
    "plt.title('Longitude Prediction')\n",
    "plt.xlabel('Sample Index')\n",
    "plt.ylabel('Longitude')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Affichage de la courbe de perte pour évaluer l'apprentissage\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
